{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and camera calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imageio: 'ffmpeg-osx-v3.2.4' was not found on your computer; downloading it now.\n",
      "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-osx-v3.2.4 (33.8 MB)\n",
      "Downloading: 8192/35458856 bytes (0.0385024/35458856 bytes (1.11024000/35458856 bytes (2.9%1646592/35458856 bytes (4.6%2252800/35458856 bytes (6.4%2859008/35458856 bytes (8.1%3465216/35458856 bytes (9.8%3956736/35458856 bytes (11.24808704/35458856 bytes (13.65414912/35458856 bytes (15.36021120/35458856 bytes (17.06627328/35458856 bytes (18.77233536/35458856 bytes (20.47839744/35458856 bytes (22.18445952/35458856 bytes (23.89052160/35458856 bytes (25.59658368/35458856 bytes (27.210264576/35458856 bytes (28.9%10870784/35458856 bytes (30.7%11476992/35458856 bytes (32.4%12083200/35458856 bytes (34.1%12673024/35458856 bytes (35.7%13295616/35458856 bytes (37.5%13836288/35458856 bytes (39.0%14475264/35458856 bytes (40.8%15065088/35458856 bytes (42.5%15671296/35458856 bytes (44.2%16195584/35458856 bytes (45.7%17031168/35458856 bytes (48.0%17637376/35458856 bytes (49.7%18243584/35458856 bytes (51.5%18849792/35458856 bytes (53.2%19472384/35458856 bytes (54.9%20111360/35458856 bytes (56.7%20717568/35458856 bytes (58.4%21323776/35458856 bytes (60.1%21929984/35458856 bytes (61.8%22536192/35458856 bytes (63.6%23142400/35458856 bytes (65.3%23748608/35458856 bytes (67.0%24354816/35458856 bytes (68.7%24961024/35458856 bytes (70.4%25567232/35458856 bytes (72.1%26198016/35458856 bytes (73.9%26779648/35458856 bytes (75.5%27385856/35458856 bytes (77.2%27992064/35458856 bytes (78.9%28598272/35458856 bytes (80.7%29204480/35458856 bytes (82.4%29810688/35458856 bytes (84.1%30416896/35458856 bytes (85.8%31023104/35458856 bytes (87.5%31629312/35458856 bytes (89.2%32235520/35458856 bytes (90.9%32841728/35458856 bytes (92.6%33447936/35458856 bytes (94.3%34054144/35458856 bytes (96.0%34660352/35458856 bytes (97.7%35266560/35458856 bytes (99.5%35458856/35458856 bytes (100.0%)\n",
      "  Done\n",
      "File saved as /Users/rush/Library/Application Support/imageio/ffmpeg/ffmpeg-osx-v3.2.4.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import cv2 \n",
    "import pickle\n",
    "import glob\n",
    "#from scipy.misc import imread, imresize\n",
    "from skimage.transform import resize\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "#file_name = 'calibration.p'\n",
    "file_name = 'Camera_Calibration/Mobius_Dashcam_Camera_Cal/1920x1080/mobius_dashcam_1920x1080_calibration.p'\n",
    "\n",
    "with open(file_name, 'rb') as f:   \n",
    "    mtx, dist = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def draw_viewing_window(image,viewing_window):\n",
    "    \"\"\"\n",
    "    Draws viewing window onto a copy of an image\n",
    "    \n",
    "    @param image:          input image\n",
    "    @param viewing_window: List of four 1x4 numpy arrays: left_line, top_line, right_line, bottom_line. \n",
    "    Each numpy array represents one of the four lines of the polygon that defines the viewing window. \n",
    "    Each numpy array contains four numbers: x1, y1, x2, y2. (x1, y1) is the starting coordinate of the \n",
    "    line. (x2, y2) is the ending coordinate of the line. \n",
    "    \n",
    "    @result image_copy: resulting image with viewing window drawn on a copy of the input image\n",
    "    \n",
    "    \"\"\"\n",
    "    image_copy = np.copy(image)\n",
    "    for line in viewing_window:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            # Draw the line onto image_copy \n",
    "            cv2.line(image_copy,(x1,y1),(x2,y2),(0,255,0),10)\n",
    "    return image_copy\n",
    "\n",
    "# Makes the viewwing window which is used by draw_viewing_window()\n",
    "def make_viewing_window(bottom_left,top_left,top_right,bottom_right):\n",
    "    \"\"\"\n",
    "    Makes the viewing window that is used by draw_viewing_window()\n",
    "    \n",
    "    @param bottom_left:     A two-element list or tuple containing the (x,y) coordinate of the bottom left point \n",
    "    of the viewing window\n",
    "    @param top_left:        A two-element list or tuple containing the (x,y) coordinate of the top left point of \n",
    "    the viewing window\n",
    "    @param top_right:       A two-element list or tuple containing the (x,y) coordinate of the top right point of \n",
    "    the viewing window\n",
    "    @param bottom_right:    A two-element linst or tuple containing the (x,y) coordinate of the bottom right point \n",
    "    of the viewing window\n",
    "    \n",
    "    @result viewing_window: List of four 1x4 numpy arrays: left_line, top_line, right_line, bottom_line. Each \n",
    "    numpy array contains four numbers: x1, y1, x2, y2. (x1, y1) is the starting coordinate of the line. (x2, y2) \n",
    "    is the ending coordinate of the line.  \n",
    "    \"\"\"\n",
    "    left_line = np.array([[bottom_left[0],bottom_left[1],top_left[0],top_left[1]]])\n",
    "    top_line = np.array([[top_left[0],top_left[1],top_right[0],top_right[1]]])\n",
    "    right_line = np.array([[top_right[0],top_right[1],bottom_right[0],bottom_right[1]]])\n",
    "    bottom_line = np.array([[bottom_right[0],bottom_right[1],bottom_left[0],bottom_left[1]]])\n",
    "    viewing_window = [left_line,top_line,right_line,bottom_line]\n",
    "\n",
    "    return viewing_window\n",
    "\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh_min=0, thresh_max=255):\n",
    "    \"\"\"\n",
    "    Applies the sobel algorithm to get the absolute derivative of an image with respect to x or y.\n",
    "    \n",
    "    @param img:           input image. MUST be an RGB image. \n",
    "    @orient:              can be 'x' which means take the derivative with respect to x. If it is anything else, it \n",
    "    will take the derivative with respect to y.\n",
    "    @sobel_kernel:        Defines the size of the sobel filter. MUST be an odd number between 3-31. The larger the \n",
    "    kernel size, the more well-defined the gradients. However, computational time will increase as well.\n",
    "    @param thresh_min:    minimum cut-off for gradient threshold. Any pixels below this value will be blacked out.\n",
    "    Default value is zero.\n",
    "    @param thresh_max:    maximum cut-off for gradient threshold. Any pixels above this value will be blacked out.\n",
    "    Default value is 255. \n",
    "    \n",
    "    @result sobel_binary: final resulting image. The resulting image is a binary image.  \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to gray-scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Take derivative with respect to x\n",
    "    if (orient == 'x'):\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_32F, 1, 0 ,ksize=sobel_kernel)\n",
    "    # Take derivative with respect to y\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_32F, 0, 1,ksize=sobel_kernel)\n",
    "    \n",
    "    # Take absolute value of derivative    \n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    \n",
    "    # Convert to 8-bit image (0 - 255)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Make copy of scaled_sobel with all zeros\n",
    "    sobel_binary = np.zeros_like(scaled_sobel)\n",
    "    \n",
    "    # Make all pixels within threshold range a value of 1\n",
    "    # Keep all other pixels as 0\n",
    "    sobel_binary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "    \n",
    "    return sobel_binary\n",
    "\n",
    "def sobel_mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    \"\"\"\n",
    "    Uses the sobel algorithm to take the magnitude of the gradient of an image\n",
    "    \n",
    "    @param img:           input image. MUST be an RGB image.\n",
    "    @sobel_kernel:        Defines the size of the sobel filter. MUST be an odd number between 3-31. The larger the \n",
    "    kernel size, the more well-defined the gradients. However, computational time will increase as well. \n",
    "    coordinate of the line. (x2, y2) is the ending coordinate of the line. \n",
    "    @param mag_thresh:    A tuple which contains the minimum and maximum gradient thresholds. (minimum, maximum).\n",
    "    \n",
    "    @result sobel_binary: the resulting image. The resulting image is a binary image. \n",
    "    \"\"\"\n",
    "    thresh_min = mag_thresh[0]\n",
    "    thresh_max = mag_thresh[1]\n",
    "    \n",
    "    # Convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Take derivatives in both x and y direction\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # Find magnitude of the gradient\n",
    "    sum_of_squares = np.square(sobelx) + np.square(sobely)\n",
    "    sobel_mag = np.power(sum_of_squares,0.5)\n",
    "    \n",
    "    # Convert to 8-bit image (0 - 255)\n",
    "    scaled_sobel = np.uint8(255*sobel_mag/np.max(sobel_mag))\n",
    "    \n",
    "    # Make a copy of sobel_mag with all zeros\n",
    "    sobel_binary = np.zeros_like(scaled_sobel)\n",
    "    \n",
    "    # Make all pixels within threshold range a value of 1\n",
    "    # Keep all other pixels as 0\n",
    "    sobel_binary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "\n",
    "    return sobel_binary\n",
    "\n",
    "# Function that applies Sobel x and y, \n",
    "# then computes the direction of the gradient\n",
    "# and applies a threshold.\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \"\"\"\n",
    "    Uses the sobel algorithm to find the direction of the gradient of an image\n",
    "\n",
    "    @param img:           input image. MUST be an RGB image. \n",
    "    @param sobel_kernel:  Defines the size of the sobel filter. MUST be an odd number between 3-31. The larger the \n",
    "    kernel size, the more well-defined the gradients. However, computational time will increase as well.\n",
    "    @param thresh:        A tuple which defines the minimum and maximum gradient thresholds. (minimum, maximum)\n",
    "\n",
    "    @result sobel_binary: the resulting image. The resulting image is a binary image.\n",
    "    \"\"\"   \n",
    "    # Min and Max Threshold Angles\n",
    "    thresh_min = thresh[0]\n",
    "    thresh_max = thresh[1]\n",
    "\n",
    "    # Convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Calculate the derivatives with respect to x and y\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    # Take absolute value of derivatives\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "\n",
    "    # Calculate angle for direction of gradient in radians\n",
    "    sobel_angle = np.arctan2(abs_sobely,abs_sobelx)\n",
    "\n",
    "    # Make a copy of sobel_angle with all zeros\n",
    "    sobel_binary = np.zeros_like(sobel_angle)\n",
    "\n",
    "    # Apply thresholding\n",
    "    sobel_binary[(sobel_angle >= thresh_min) & (sobel_angle <= thresh_max)] = 1\n",
    "\n",
    "    return sobel_binary\n",
    "\n",
    "def region_of_interest(gray, limit_look_ahead):\n",
    "  \"\"\"\n",
    "  Applies a region of interest to a gray-scale image. This function is frequently modified. Currently, it just\n",
    "  blacks out certain rectangular regions. \n",
    "  \n",
    "  @param gray:             Input image. MUST be a gray-scale image\n",
    "  @param limit_look_ahead: A floating point number that must be between 0.0 and 1.0. It determines what percentage\n",
    "  of the top of the image to black out. \n",
    "  \n",
    "  @result copy:            The resulting gray-scale image after the region of interest was applied.\n",
    "  \"\"\"\n",
    "  copy = np.copy(gray) + 1\n",
    "  #copy[:, :225] = 0 \n",
    "  copy[650:1080, 800:1500] = 0 \n",
    "  #copy[0:800, 0:350] = 0\n",
    "  copy[:int(gray.shape[0]*limit_look_ahead), :] = 0\n",
    "  return copy \n",
    "\n",
    "def combined_threshold(image, gradx_low=40, gradx_high=255, mag_low=40, mag_high=255, dir_low=0.7, dir_high=1.3, \\\n",
    "                       s_low=140, s_high=255, l_low=40, l_high=255,  l_agr=205, kernel_size=3):\n",
    "    \"\"\"\n",
    "    @param image:       input image. MUST be RGB.\n",
    "    @param gradx_low:   minimum threshold value for applying gradient with respect to x\n",
    "    @param gradx_high:  maximum threshold value for applying gradient with respect to x\n",
    "    @param mag_low:     minimum threshold value for finding magnitude of the gradient\n",
    "    @param mag_high:    maximum threshold value for finding magnitude of the gradient\n",
    "    @param dir_low:     minimum threshold value for fidning the direction of the gradient\n",
    "    @param dir_high:    maximum threshold value for finding the direction of the gradient\n",
    "    @param s_low:       minimum threshold value for applying color thresholding to S channel from HLS color space\n",
    "    @param s_high:      maximum threshold value for applying color thresholding to S channel from HLS color space\n",
    "    @param l_low:       minimum threshold value for applying color thresholding to L channel from HLS color space\n",
    "    @param l_high:      maximum threshold value for applying color thresholding to L channel from HLS color space\n",
    "    @param l_agr:       minimum threshold value for applying second round of color thresholding to L channel from\n",
    "    HLS color space. Any pixels above this value will be kept. The idea is that very bright pixels during daylight\n",
    "    driving and even during night driving due to street lights and reflectors on the road are most likely lane \n",
    "    line pixels.\n",
    "    @param kernel_size: Defines the size of the sobel filter. MUST be an odd number between 3-31. The larger the \n",
    "    kernel size, the more well-defined the gradients. However, computational time will increase as well.\n",
    "    \n",
    "    @result combined:   The resulting binary image after appling a combination of color and gradient \n",
    "    thresholds\n",
    "    \"\"\"\n",
    "    # Convert image to HLS color space\n",
    "    image_hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    \n",
    "    # Extract S channel\n",
    "    S = image_hls[:,:,2]\n",
    "    \n",
    "    # Apply gradient thresholding to S channel\n",
    "    S_thresholded = np.zeros_like(S)\n",
    "    thresh_S = (s_low, s_high)\n",
    "    S_thresholded[(S >= thresh_S[0]) & (S <= thresh_S[1])] = 1\n",
    "    \n",
    "    # Extract L channel\n",
    "    L = image_hls[:,:,1]\n",
    "    \n",
    "    # Apply gradient thresholding to L channel\n",
    "    L_thresholded = np.zeros_like(L)\n",
    "    thresh_L = (l_low, l_high)\n",
    "    L_thresholded[(L >=thresh_L[0]) & (L <= thresh_L[1])] = 1\n",
    "    \n",
    "    # Apply second round of gradient thresholding using L channel\n",
    "    # If a pixel is extremely bright, above l_agr, keep it. \n",
    "    thresh_L_agr = l_agr\n",
    "    L_thresholded2 = np.zeros_like(L)\n",
    "    L_thresholded2[L>=thresh_L_agr] = 1\n",
    "    \n",
    "    # Apply gradient with respect to x\n",
    "    gradx = abs_sobel_thresh(image, orient='x', sobel_kernel=kernel_size, thresh_min=gradx_low,\n",
    "                             thresh_max=gradx_high)\n",
    "\n",
    "    # Find magnitude of the gradient\n",
    "    mag_binary = sobel_mag_thresh(image, sobel_kernel=kernel_size, mag_thresh=(mag_low,mag_high))\n",
    "    \n",
    "    # Find direction of the gradient\n",
    "    dir_binary = dir_threshold(image,sobel_kernel=kernel_size,thresh=(0.7,1.3))\n",
    "\n",
    "    # Create blank image for combining all the color and gradient thresholds\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    \n",
    "    # Combine all color and gradient thresholds\n",
    "    combined[((mag_binary == 1) & (dir_binary == 1) & (gradx == 1)) | \\\n",
    "             (((S_thresholded == 1) & (L_thresholded == 1)) | (L_thresholded2 == 1))] = 1\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def apply_birdseye(image,source_points,dest_points):\n",
    "    \"\"\"\n",
    "    Applies perspective transformation to input image and obtains a birds-eye view of the image.\n",
    "    \n",
    "    @param image:         input image. The input image should be undistorted.\n",
    "    @param source_points: A 4x2 float32 numpy array where each row defines one of the four points of the source\n",
    "    points used for the perspective transformation. This is the order: \n",
    "    Top left, bottom left, bottom right, top right.\n",
    "    @param dest_points:   A 4x2 float32 numpy array where each row defines one of the four points of the \n",
    "    destination points used for the perspective transformation. \n",
    "    \n",
    "    @result birds_eye_image: the resulting birds-eye view image. \n",
    "    \"\"\"\n",
    "    M = cv2.getPerspectiveTransform(source_points,dest_points)\n",
    "    Minv = cv2.getPerspectiveTransform(dest_points,source_points)\n",
    "    img_size = (image.shape[1],image.shape[0])\n",
    "\n",
    "    birds_eye_image = cv2.warpPerspective(image, M, img_size,\n",
    "                                          flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return birds_eye_image\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Takes in an image and performs lane line detection on it. It can only see the two lane lines that define\n",
    "    the car's current lane\n",
    "    \n",
    "    @param: input image. MUST be an RGB image\n",
    "    \n",
    "    @result result: the resulting image with the lane drawn on top.     \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"Begin transforming image into a binary birds-eye view with only lane line pixels visible\"\"\"\n",
    "    \n",
    "    # Source points for mobius 1920x1080\n",
    "    src_top_left = [790,460]\n",
    "    src_bottom_left = [290,720]\n",
    "    src_bottom_right = [1550,720]\n",
    "    src_top_right = [1050,460]\n",
    "    source_points = np.float32([src_top_left,src_bottom_left,src_bottom_right,\n",
    "                     src_top_right])\n",
    "\n",
    "    # Destination points for mobius 1920x1080\n",
    "    left = 0\n",
    "    right = 1920\n",
    "    bottom = 0\n",
    "    top = 1080\n",
    "    dest_top_left = [left,bottom]\n",
    "    dest_bottom_left = [left,top]\n",
    "    dest_bottom_right = [right,top]\n",
    "    dest_top_right = [right,bottom]\n",
    "    dest_points = np.float32([dest_top_left,dest_bottom_left,dest_bottom_right,\n",
    "                              dest_top_right])\n",
    "\n",
    "    # Get the inverse perspective matrix to be used later to warp predicted lane back onto original image\n",
    "    Minv = cv2.getPerspectiveTransform(dest_points,source_points)\n",
    "    \n",
    "    # Undistort image\n",
    "    image_undistorted = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    \n",
    "    # Convert to birds-eye view\n",
    "    birdseye_view = apply_birdseye(image_undistorted,source_points,dest_points)\n",
    "    \n",
    "    # Applying Combined Color and Gradient Thresholding to Birds-Eye View Image\n",
    "    combined = combined_threshold(birdseye_view, gradx_low=40, gradx_high=255, mag_low=40, mag_high=255, dir_low=0.7, dir_high=1.3, \\\n",
    "                       s_low=100, s_high=255, l_low=120, l_high=255,  l_agr=205, kernel_size=3)\n",
    "    \n",
    "    # Black out top 40% of image\n",
    "    limit_look_ahead = 0.4\n",
    "    binary_warped = np.logical_and(combined,region_of_interest(combined,limit_look_ahead=limit_look_ahead)).astype(np.uint8)\n",
    "    \n",
    "    \"\"\"Begin the histogram-based window search\"\"\"\n",
    "    \n",
    "    # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "    # Take a histogram (column-wise) of the bottom portion of the image\n",
    "    \n",
    "    # What faction of the bottom of the image you wish to perform the histogram on (must be between 0.0 and 1.0)\n",
    "    bottom_fraction = 0.25\n",
    "    \n",
    "    # Get the original height of the image\n",
    "    \n",
    "    height = binary_warped.shape[0]\n",
    "    width = binary_warped.shape[1]\n",
    "    \n",
    "    # Perform the histogram on some percentage of the bottom of the image\n",
    "    histogram = np.sum(binary_warped[int(height - height*bottom_fraction):,:], axis=0)\n",
    "\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    windows = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Determines the height of the windows\n",
    "    nwindows = 8\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    lefty_current = image.shape[0] - window_height//2\n",
    "    righty_current = image.shape[0] - window_height//2\n",
    "    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 200 # best is 200\n",
    "    \n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 500 # best is 50 (500)\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    # Re-center window based on both x and y position\n",
    "    \n",
    "    # Initialize top, bottom, left, and right boundaries of left and right search windows\n",
    "    win_yleft_low = lefty_current + window_height//2\n",
    "    win_yleft_high = lefty_current - window_height//2\n",
    "    win_xleft_low = leftx_current - margin\n",
    "    win_xleft_high = leftx_current + margin\n",
    "    win_yright_low = righty_current + window_height//2\n",
    "    win_yright_high = righty_current - window_height//2\n",
    "    win_xright_low = rightx_current - margin\n",
    "    win_xright_high = rightx_current + margin\n",
    "    \n",
    "    # Initialize the direction the left and right window searches move in\n",
    "    left_dx = 0 # number of pixels to move in the x-direction for left window search\n",
    "    left_dy = -1 # number of pixels to move in the y-direction for left window search\n",
    "    right_dx = 0 # number of pixels to move in the x-direction for right window search\n",
    "    right_dy = -1 # number of pixels to move in the y-direction for right window search\n",
    "    \n",
    "    # margin of wiggle room before stopping window search when it exits the side of the image\n",
    "    side_margin = 1.0 #1.25\n",
    "    # margin of wiggle room before stopping window search when it crosses into other half of image\n",
    "    middle_margin = 5.0 # 1.0\n",
    "    \n",
    "    n_left_windows = 0 # Initialize the number of left windows used\n",
    "    n_right_windows = 0 # Initialize the number of right windows used\n",
    "    min_n_windows = 100 # min number of windows before terminating window search\n",
    "    \n",
    "    \"\"\"\n",
    "    While (the left window search is within the left side of the image (plus some wiggle room) OR minimum number\n",
    "    of left windows have been reached) OR (the right window search is within the right side of the image (plus\n",
    "    some wiggle room) OR minimum number of right windows have been reached)\n",
    "    \"\"\"\n",
    "    while ((((win_xleft_low >= -1*(margin//2)*side_margin) & (win_xleft_high <= (image.shape[1]//2 + ((margin//2)*middle_margin))) & (win_yleft_high > 0)) | (n_left_windows < min_n_windows)) | \\\n",
    "            (((win_xright_low >= (image.shape[1]//2 - ((margin//2)*middle_margin))) & (win_xright_high <= (image.shape[1] + (margin//2)*side_margin)) & (win_yright_high > 0)) | (n_right_windows < min_n_windows))):\n",
    "    \n",
    "        # Do left lane line\n",
    "        # Find left, right, top, bottom, boundaries of window\n",
    "        win_yleft_low = lefty_current + window_height//2\n",
    "        win_yleft_high = lefty_current - window_height//2\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "\n",
    "        # Stop performing left window search if left lane line exits left side of image\n",
    "        if (((win_xleft_low >= -1*(margin//2)*side_margin) & \\\n",
    "             (win_xleft_high <= image.shape[1]//2 + (margin//2)*middle_margin)) | \\\n",
    "             (n_left_windows < min_n_windows)): # 1.5\n",
    "            n_left_windows += 1\n",
    "            # Draw window\n",
    "            cv2.rectangle(windows,(win_xleft_low,win_yleft_low),(win_xleft_high,win_yleft_high),\n",
    "            (0,255,0), 2) \n",
    "            # Get indicies of nonzero pixels within window\n",
    "            good_left_inds = ((nonzeroy < win_yleft_low) & (nonzeroy >= win_yleft_high) & \n",
    "            (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            # Append these indicies to list of left lane line indicies\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                # Always re-center x position; let new x position go to the left or right\n",
    "                leftx_previous = leftx_current\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "                left_dx = leftx_current - leftx_previous\n",
    "                # Only re-center y position if the new center is higher up on the image than the previous center\n",
    "                # higher up on the image means a smaller y value\n",
    "                # 0 y value is at the top of the image\n",
    "                if (np.int(np.mean(nonzeroy[good_left_inds])) < lefty_current):\n",
    "                    lefty_previous = lefty_current \n",
    "                    lefty_current = np.int(np.mean(nonzeroy[good_left_inds]))\n",
    "                    left_dy = lefty_current - lefty_previous\n",
    "                # if re-centering causes y to go down (higher y value), do not let window search to go back down\n",
    "                # keep window search moving in previous y direction\n",
    "                else:\n",
    "                    lefty_current += left_dy\n",
    "            # if mininum number of pixels was not found, keep moving in previous x and y direction\n",
    "            else:\n",
    "                leftx_current += left_dx\n",
    "                lefty_current += left_dy\n",
    "            \n",
    "        \n",
    "        # Do right lane line\n",
    "        # Find left and right boundaries of window\n",
    "        win_yright_low = righty_current + window_height//2\n",
    "        win_yright_high = righty_current - window_height//2\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "\n",
    "        # Stop performing right window search if right lane line exits right side of image\n",
    "        if (((win_xright_high <= image.shape[1] + (margin//2)*side_margin) & \\\n",
    "             (win_xright_low >=(image.shape[1]//2 - (margin//2)*middle_margin)) | \\\n",
    "             (n_right_windows < min_n_windows))): # 1.5\n",
    "            n_right_windows += 1\n",
    "            # Draw Window\n",
    "            cv2.rectangle(windows,(win_xright_low,win_yright_low),(win_xright_high,win_yright_high),\n",
    "            (0,255,0), 2) \n",
    "            # Get indicies of nonzero pixels within window\n",
    "            good_right_inds = ((nonzeroy < win_yright_low) & (nonzeroy >= win_yright_high) & \n",
    "            (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indicies to list of right lane line indicies\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            # if you found > minpix pixels, recenter next window on mean x-position\n",
    "            if len(good_right_inds) > minpix: \n",
    "                rightx_previous = rightx_current\n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "                right_dx = rightx_current - rightx_previous\n",
    "                # only re-center y position if the new y-position is higher up (lower y value)\n",
    "                if (np.int(np.mean(nonzeroy[good_right_inds])) < righty_current):\n",
    "                    righty_previous = righty_current\n",
    "                    righty_current = np.int(np.mean(nonzeroy[good_right_inds]))\n",
    "                    right_dy = righty_current - righty_previous\n",
    "                # if re-centering causes y to go down (higher y value), do not let window search to go back down\n",
    "                # keep window search moving in previous y direction\n",
    "                else:\n",
    "                    righty_current += right_dy\n",
    "            # if mininum number of pixels was not found, keep moving in previous x and y direction\n",
    "            else:\n",
    "                rightx_current += right_dx\n",
    "                righty_current += right_dy\n",
    "\n",
    "    \"\"\"Begin finding best-fit line for both lane lines\"\"\"\n",
    "    \n",
    "    # Concatenate the arrays of indices\n",
    "    if (len(left_lane_inds) > 0):\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    if (len(right_lane_inds) > 0):\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    left_lane_inds = np.unique(left_lane_inds) # get rid of repeats\n",
    "    right_lane_inds = np.unique(right_lane_inds) # get rid of repeats\n",
    "    \n",
    "    # Temporary fix to account for rare case when no lane line pixels were found\n",
    "    if (len(left_lane_inds) > 0):\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "    else:\n",
    "        leftx = []\n",
    "        lefty = []\n",
    "    if (len(right_lane_inds) > 0):\n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "    else:\n",
    "        rightx = []\n",
    "        righty = []\n",
    "      \n",
    "    # Factor by which to shift starting y position of best-fit line for the lane lines\n",
    "    # 1.0 means left-lane line starts at the bottom of the image; \n",
    "    shift_lane_up = 1.0 # originally 1.25\n",
    "    # y values for plotting the best-fit lines\n",
    "    ploty = np.linspace(limit_look_ahead*image.shape[0], (binary_warped.shape[0]-1)*shift_lane_up, binary_warped.shape[0] )\n",
    "    \n",
    "    # initial \n",
    "    left_fit = [0,0,0]\n",
    "    right_fit = [0,0, image.shape[1]]\n",
    "    n = 2 # degree of polynomial for best-fit line (2 works best)\n",
    "    \n",
    "    # Fit a second-order polynomial for left lane line\n",
    "    if ((len(lefty) > 0) & (len(leftx) > 0)):\n",
    "        # Use RANSAC algorithm to find best-fit line for left lane line\n",
    "        left_model = make_pipeline(PolynomialFeatures(n), RANSACRegressor(random_state=42))\n",
    "        left_model.fit(lefty.reshape(-1,1), leftx)\n",
    "\n",
    "        \"\"\"\n",
    "        For some reason, the one line of code below fails to return the correct coefficients of the best-fit \n",
    "        line. It only returns 2/3 of the coefficients. One coefficient is always zero...So the coefficients\n",
    "        have to be re-calculated using np.polyfit. We need the coefficients to calculate radius of curvature\n",
    "        #left_fit = left_model.named_steps['ransacregressor'].estimator_.coef_\n",
    "        \"\"\"\n",
    "        \n",
    "        left_fitx = left_model.predict(ploty.reshape(-1,1))\n",
    "        left_fit = np.polyfit(ploty,left_fitx,2)\n",
    "              \n",
    "    else:\n",
    "        left_fit = [0,0,0]\n",
    "     \n",
    "    # Fit a second-order polynomial for right lane line\n",
    "    if ((len(righty) > 0) & (len(rightx) > 0)):\n",
    "        \n",
    "        right_model = make_pipeline(PolynomialFeatures(n), RANSACRegressor(random_state=42))\n",
    "        right_model.fit(righty.reshape(-1,1), rightx)\n",
    "        \n",
    "        \"\"\"\n",
    "        For some reason, the one line of code below fails to return the correct coefficients of the best-fit \n",
    "        line. It only returns 2/3 of the coefficients. One coefficient is always zero...So the coefficients\n",
    "        have to be re-calculated using np.polyfit. We need the coefficients to calculate radius of curvature\n",
    "        #right_fit = right_model.named_steps['ransacregressor'].estimator_.coef_\n",
    "        \"\"\"\n",
    "        \n",
    "        right_fitx = right_model.predict(ploty.reshape(-1,1))\n",
    "        right_fit = np.polyfit(ploty,right_fitx,2)\n",
    "                \n",
    "    else:\n",
    "        right_fit = [0,image.shape[1]]\n",
    "\n",
    "    \"\"\"Visualize results and calculate radii of curvature and ceter offset\"\"\"\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    # Temporary fix for rare case when no lane line pixels were found\n",
    "    if (len(left_lane_inds) > 0):\n",
    "        windows[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    if (len(right_lane_inds) > 0):\n",
    "        windows[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    # Create an image to draw on the best-fit line\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # Create an image that will have the best-fit lines colored in and then overlay it on top of out_img\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    \n",
    "    # Color in left and right line pixels on out_img\n",
    "    # Temporary fix for rare case when non lane line pixels were found\n",
    "    if (len(left_lane_inds) > 0):\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    if (len(right_lane_inds) > 0):\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                                  ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                                  ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "\n",
    "    # Draw the best-fit lines and overlay onto out_img\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    best_fit = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "\n",
    "    # Calculating radius of curvature at bottom of picture in pixels\n",
    "    y_eval = np.max(ploty)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    yinches_per_pix = 8/480\n",
    "    xinches_per_pix = 5/640\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    if (((len(lefty) != 0) & (len(leftx) != 0))):\n",
    "        left_fit_cr = np.polyfit(lefty*yinches_per_pix, leftx*xinches_per_pix, 2)\n",
    "    else:\n",
    "        left_fit_cr = [0,0,0]\n",
    "    if (((len(righty) != 0) & (len(rightx) != 0))):\n",
    "        right_fit_cr = np.polyfit(righty*yinches_per_pix, rightx*xinches_per_pix, 2)\n",
    "    else:\n",
    "        right_fit_cr = [0,0,1920*xinches_per_pix]\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*yinches_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*yinches_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "    #warp_zero = np.zeros((int(height*1.25),width),np.uint8)\n",
    "    \n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    vehicle_center = width/2\n",
    "    left_lane_position = (left_fit[0] * (height**2)) + (left_fit[1] * height) + left_fit[2]\n",
    "    right_lane_position = (right_fit[0] * (height**2)) + (right_fit[1] * height) + right_fit[2]\n",
    "    actual_center = left_lane_position + (right_lane_position - left_lane_position) / 2\n",
    "    vehicle_position = (actual_center - vehicle_center) * xinches_per_pix\n",
    "    vehicle_position = round((vehicle_position),2)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    #cv2.putText(result, \"Left Radius Of Curvature: \" + str(round(left_curverad,0)), (50, 50), font, 1.1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    #cv2.putText(result, \"Right Radius Of Curvature: \" + str(round(right_curverad,0)), (50, 100), font, 1.1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    if (vehicle_position < 0):\n",
    "        vehicle_position_str = str(np.absolute(vehicle_position)) + \" inches right of center\"\n",
    "    elif (vehicle_position > 0):\n",
    "        vehicle_position_str = str(np.absolute(vehicle_position)) + \" inches left of center\"\n",
    "    else:\n",
    "        vehicle_position_str =  \"on center\"\n",
    "    #cv2.putText(result, vehicle_position_str, (50,150), font, 1.1, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Draw the following four images on top to help visualize and debug algorithm\n",
    "    birdseye_view = resize(birdseye_view, (height//4,width//4))*255\n",
    "    binary_warped = resize(binary_warped, (height//4,width//4))*255\n",
    "    windows = resize(windows, (height//4,width//4))*255\n",
    "    best_fit = resize(best_fit, (height//4,width//4))*255\n",
    "    \n",
    "    binary_warped_color = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "\n",
    "    offset = [0, int(image.shape[1]*0.25), int(image.shape[1]*0.5), int(image.shape[1]*0.75)]\n",
    "    width, height = image.shape[1]//4, image.shape[0]//4\n",
    "    \n",
    "    result[:height, offset[0]: offset[0] + width] = birdseye_view\n",
    "    result[:height, offset[1]: offset[1] + width] = binary_warped_color\n",
    "    result[:height, offset[2]: offset[2] + width] = windows\n",
    "    result[:height, offset[3]: offset[3] + width] = best_fit\n",
    "\n",
    "    return result\n",
    "print('done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video Real_Car_Images_And_Videos/Bay_Area_Output_Videos/output4.mp4\n",
      "[MoviePy] Writing video Real_Car_Images_And_Videos/Bay_Area_Output_Videos/output4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/151 [00:00<02:07,  1.18it/s]\u001b[A\n",
      "  1%|▏         | 2/151 [00:01<02:04,  1.19it/s]\u001b[A\n",
      "  2%|▏         | 3/151 [00:02<02:04,  1.19it/s]\u001b[A\n",
      "  3%|▎         | 4/151 [00:03<02:03,  1.19it/s]\u001b[A\n",
      "  3%|▎         | 5/151 [00:04<02:04,  1.17it/s]\u001b[A\n",
      "  4%|▍         | 6/151 [00:05<02:04,  1.16it/s]\u001b[A\n",
      "  5%|▍         | 7/151 [00:06<02:03,  1.16it/s]\u001b[A\n",
      "  5%|▌         | 8/151 [00:06<02:01,  1.17it/s]\u001b[A\n",
      "  6%|▌         | 9/151 [00:07<02:02,  1.16it/s]\u001b[A\n",
      "  7%|▋         | 10/151 [00:08<02:00,  1.17it/s]\u001b[A\n",
      "  7%|▋         | 11/151 [00:09<01:58,  1.19it/s]\u001b[A\n",
      "  8%|▊         | 12/151 [00:10<01:56,  1.20it/s]\u001b[A\n",
      "  9%|▊         | 13/151 [00:10<01:54,  1.21it/s]\u001b[A\n",
      "  9%|▉         | 14/151 [00:11<01:52,  1.22it/s]\u001b[A\n",
      " 10%|▉         | 15/151 [00:12<01:50,  1.23it/s]\u001b[A\n",
      " 11%|█         | 16/151 [00:12<01:49,  1.24it/s]\u001b[A\n",
      " 11%|█▏        | 17/151 [00:13<01:47,  1.24it/s]\u001b[A\n",
      " 12%|█▏        | 18/151 [00:14<01:46,  1.25it/s]\u001b[A\n",
      " 13%|█▎        | 19/151 [00:15<01:44,  1.26it/s]\u001b[A\n",
      " 13%|█▎        | 20/151 [00:15<01:43,  1.27it/s]\u001b[A\n",
      " 14%|█▍        | 21/151 [00:16<01:42,  1.27it/s]\u001b[A\n",
      " 15%|█▍        | 22/151 [00:17<01:41,  1.28it/s]\u001b[A\n",
      " 15%|█▌        | 23/151 [00:17<01:39,  1.28it/s]\u001b[A\n",
      " 16%|█▌        | 24/151 [00:18<01:38,  1.29it/s]\u001b[A\n",
      " 17%|█▋        | 25/151 [00:19<01:37,  1.29it/s]\u001b[A\n",
      " 17%|█▋        | 26/151 [00:20<01:36,  1.29it/s]\u001b[A\n",
      " 18%|█▊        | 27/151 [00:20<01:35,  1.29it/s]\u001b[A\n",
      " 19%|█▊        | 28/151 [00:21<01:34,  1.30it/s]\u001b[A\n",
      " 19%|█▉        | 29/151 [00:22<01:33,  1.30it/s]\u001b[A\n",
      " 20%|█▉        | 30/151 [00:23<01:32,  1.30it/s]\u001b[A\n",
      " 21%|██        | 31/151 [00:23<01:31,  1.31it/s]\u001b[A\n",
      " 21%|██        | 32/151 [00:24<01:30,  1.31it/s]\u001b[A\n",
      " 22%|██▏       | 33/151 [00:25<01:29,  1.31it/s]\u001b[A\n",
      " 23%|██▎       | 34/151 [00:25<01:29,  1.31it/s]\u001b[A\n",
      " 23%|██▎       | 35/151 [00:26<01:28,  1.31it/s]\u001b[A\n",
      " 24%|██▍       | 36/151 [00:27<01:28,  1.31it/s]\u001b[A\n",
      " 25%|██▍       | 37/151 [00:28<01:27,  1.30it/s]\u001b[A\n",
      " 25%|██▌       | 38/151 [00:29<01:26,  1.30it/s]\u001b[A\n",
      " 26%|██▌       | 39/151 [00:29<01:25,  1.30it/s]\u001b[A\n",
      " 26%|██▋       | 40/151 [00:30<01:25,  1.30it/s]\u001b[A\n",
      " 27%|██▋       | 41/151 [00:31<01:24,  1.30it/s]\u001b[A\n",
      " 28%|██▊       | 42/151 [00:32<01:24,  1.29it/s]\u001b[A\n",
      " 28%|██▊       | 43/151 [00:33<01:23,  1.29it/s]\u001b[A\n",
      " 29%|██▉       | 44/151 [00:34<01:22,  1.29it/s]\u001b[A\n",
      " 30%|██▉       | 45/151 [00:34<01:22,  1.29it/s]\u001b[A\n",
      " 30%|███       | 46/151 [00:35<01:21,  1.29it/s]\u001b[A\n",
      " 31%|███       | 47/151 [00:36<01:20,  1.29it/s]\u001b[A\n",
      " 32%|███▏      | 48/151 [00:37<01:20,  1.29it/s]\u001b[A\n",
      " 32%|███▏      | 49/151 [00:38<01:19,  1.29it/s]\u001b[A\n",
      " 33%|███▎      | 50/151 [00:38<01:18,  1.28it/s]\u001b[A\n",
      " 34%|███▍      | 51/151 [00:39<01:18,  1.28it/s]\u001b[A\n",
      " 34%|███▍      | 52/151 [00:40<01:17,  1.28it/s]\u001b[A\n",
      " 35%|███▌      | 53/151 [00:41<01:17,  1.27it/s]\u001b[A\n",
      " 36%|███▌      | 54/151 [00:42<01:16,  1.27it/s]\u001b[A\n",
      " 36%|███▋      | 55/151 [00:43<01:15,  1.27it/s]\u001b[A\n",
      " 37%|███▋      | 56/151 [00:44<01:15,  1.27it/s]\u001b[A\n",
      " 38%|███▊      | 57/151 [00:45<01:14,  1.27it/s]\u001b[A\n",
      " 38%|███▊      | 58/151 [00:45<01:13,  1.26it/s]\u001b[A\n",
      " 39%|███▉      | 59/151 [00:46<01:12,  1.27it/s]\u001b[A\n",
      " 40%|███▉      | 60/151 [00:47<01:11,  1.27it/s]\u001b[A\n",
      " 40%|████      | 61/151 [00:48<01:11,  1.26it/s]\u001b[A\n",
      " 41%|████      | 62/151 [00:48<01:10,  1.27it/s]\u001b[A\n",
      " 42%|████▏     | 63/151 [00:49<01:09,  1.27it/s]\u001b[A\n",
      " 42%|████▏     | 64/151 [00:50<01:08,  1.27it/s]\u001b[A\n",
      " 43%|████▎     | 65/151 [00:51<01:07,  1.27it/s]\u001b[A\n",
      " 44%|████▎     | 66/151 [00:51<01:06,  1.27it/s]\u001b[A\n",
      " 44%|████▍     | 67/151 [00:52<01:06,  1.27it/s]\u001b[A\n",
      " 45%|████▌     | 68/151 [00:53<01:05,  1.27it/s]\u001b[A\n",
      " 46%|████▌     | 69/151 [00:54<01:04,  1.27it/s]\u001b[A\n",
      " 46%|████▋     | 70/151 [00:55<01:03,  1.27it/s]\u001b[A\n",
      " 47%|████▋     | 71/151 [00:56<01:03,  1.26it/s]\u001b[A\n",
      " 48%|████▊     | 72/151 [00:56<01:02,  1.26it/s]\u001b[A\n",
      " 48%|████▊     | 73/151 [00:57<01:01,  1.26it/s]\u001b[A\n",
      " 49%|████▉     | 74/151 [00:58<01:01,  1.26it/s]\u001b[A\n",
      " 50%|████▉     | 75/151 [00:59<01:00,  1.26it/s]\u001b[A\n",
      " 50%|█████     | 76/151 [01:00<00:59,  1.25it/s]\u001b[A\n",
      " 51%|█████     | 77/151 [01:01<00:59,  1.25it/s]\u001b[A\n",
      " 52%|█████▏    | 78/151 [01:02<00:58,  1.25it/s]\u001b[A\n",
      " 52%|█████▏    | 79/151 [01:03<00:57,  1.25it/s]\u001b[A\n",
      " 53%|█████▎    | 80/151 [01:04<00:57,  1.25it/s]\u001b[A\n",
      " 54%|█████▎    | 81/151 [01:05<00:56,  1.24it/s]\u001b[A\n",
      " 54%|█████▍    | 82/151 [01:06<00:55,  1.24it/s]\u001b[A\n",
      " 55%|█████▍    | 83/151 [01:07<00:55,  1.24it/s]\u001b[A\n",
      " 56%|█████▌    | 84/151 [01:08<00:54,  1.23it/s]\u001b[A\n",
      " 56%|█████▋    | 85/151 [01:08<00:53,  1.23it/s]\u001b[A\n",
      " 57%|█████▋    | 86/151 [01:09<00:52,  1.23it/s]\u001b[A\n",
      " 58%|█████▊    | 87/151 [01:10<00:52,  1.23it/s]\u001b[A\n",
      " 58%|█████▊    | 88/151 [01:11<00:51,  1.23it/s]\u001b[A\n",
      " 59%|█████▉    | 89/151 [01:12<00:50,  1.23it/s]\u001b[A\n",
      " 60%|█████▉    | 90/151 [01:13<00:49,  1.23it/s]\u001b[A\n",
      " 60%|██████    | 91/151 [01:14<00:48,  1.23it/s]\u001b[A\n",
      " 61%|██████    | 92/151 [01:14<00:48,  1.23it/s]\u001b[A\n",
      " 62%|██████▏   | 93/151 [01:15<00:47,  1.23it/s]\u001b[A\n",
      " 62%|██████▏   | 94/151 [01:16<00:46,  1.23it/s]\u001b[A\n",
      " 63%|██████▎   | 95/151 [01:17<00:45,  1.23it/s]\u001b[A\n",
      " 64%|██████▎   | 96/151 [01:17<00:44,  1.23it/s]\u001b[A\n",
      " 64%|██████▍   | 97/151 [01:18<00:43,  1.23it/s]\u001b[A\n",
      " 65%|██████▍   | 98/151 [01:19<00:42,  1.23it/s]\u001b[A\n",
      " 66%|██████▌   | 99/151 [01:20<00:42,  1.24it/s]\u001b[A\n",
      " 66%|██████▌   | 100/151 [01:20<00:41,  1.24it/s]\u001b[A\n",
      " 67%|██████▋   | 101/151 [01:21<00:40,  1.24it/s]\u001b[A\n",
      " 68%|██████▊   | 102/151 [01:22<00:39,  1.24it/s]\u001b[A\n",
      " 68%|██████▊   | 103/151 [01:23<00:38,  1.24it/s]\u001b[A\n",
      " 69%|██████▉   | 104/151 [01:23<00:37,  1.24it/s]\u001b[A\n",
      " 70%|██████▉   | 105/151 [01:24<00:37,  1.24it/s]\u001b[A\n",
      " 70%|███████   | 106/151 [01:25<00:36,  1.24it/s]\u001b[A\n",
      " 71%|███████   | 107/151 [01:26<00:35,  1.24it/s]\u001b[A\n",
      " 72%|███████▏  | 108/151 [01:26<00:34,  1.24it/s]\u001b[A\n",
      " 72%|███████▏  | 109/151 [01:27<00:33,  1.24it/s]\u001b[A\n",
      " 73%|███████▎  | 110/151 [01:28<00:32,  1.24it/s]\u001b[A\n",
      " 74%|███████▎  | 111/151 [01:29<00:32,  1.24it/s]\u001b[A\n",
      " 74%|███████▍  | 112/151 [01:29<00:31,  1.25it/s]\u001b[A\n",
      " 75%|███████▍  | 113/151 [01:30<00:30,  1.25it/s]\u001b[A\n",
      " 75%|███████▌  | 114/151 [01:31<00:29,  1.25it/s]\u001b[A\n",
      " 76%|███████▌  | 115/151 [01:32<00:28,  1.25it/s]\u001b[A\n",
      " 77%|███████▋  | 116/151 [01:32<00:28,  1.25it/s]\u001b[A\n",
      " 77%|███████▋  | 117/151 [01:33<00:27,  1.25it/s]\u001b[A\n",
      " 78%|███████▊  | 118/151 [01:34<00:26,  1.25it/s]\u001b[A\n",
      " 79%|███████▉  | 119/151 [01:35<00:25,  1.25it/s]\u001b[A\n",
      " 79%|███████▉  | 120/151 [01:36<00:24,  1.25it/s]\u001b[A\n",
      " 80%|████████  | 121/151 [01:37<00:24,  1.25it/s]\u001b[A\n",
      " 81%|████████  | 122/151 [01:37<00:23,  1.25it/s]\u001b[A\n",
      " 81%|████████▏ | 123/151 [01:38<00:22,  1.25it/s]\u001b[A\n",
      " 82%|████████▏ | 124/151 [01:39<00:21,  1.24it/s]\u001b[A\n",
      " 83%|████████▎ | 125/151 [01:40<00:20,  1.24it/s]\u001b[A\n",
      " 83%|████████▎ | 126/151 [01:41<00:20,  1.24it/s]\u001b[A\n",
      " 84%|████████▍ | 127/151 [01:42<00:19,  1.24it/s]\u001b[A\n",
      " 85%|████████▍ | 128/151 [01:43<00:18,  1.24it/s]\u001b[A\n",
      " 85%|████████▌ | 129/151 [01:44<00:17,  1.24it/s]\u001b[A\n",
      " 86%|████████▌ | 130/151 [01:44<00:16,  1.24it/s]\u001b[A\n",
      " 87%|████████▋ | 131/151 [01:45<00:16,  1.24it/s]\u001b[A\n",
      " 87%|████████▋ | 132/151 [01:46<00:15,  1.24it/s]\u001b[A\n",
      " 88%|████████▊ | 133/151 [01:47<00:14,  1.24it/s]\u001b[A\n",
      " 89%|████████▊ | 134/151 [01:48<00:13,  1.24it/s]\u001b[A\n",
      " 89%|████████▉ | 135/151 [01:49<00:12,  1.24it/s]\u001b[A\n",
      " 90%|█████████ | 136/151 [01:50<00:12,  1.23it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "project_output = 'Real_Car_Images_And_Videos/Bay_Area_Output_Videos/output4.mp4'\n",
    "clip1 = VideoFileClip(\"Real_Car_Images_And_Videos/Bay_Area_Videos/Highway_280_3-31-2018.mp4\").subclip(67,72);\n",
    "#clip1 = VideoFileClip(\"Real_Car_Images_And_Videos/Bay_Area_Videos/Highway_280_3-31-2018.mp4\");\n",
    "white_clip = clip1.fl_image(process_image) \n",
    "%time white_clip.write_videofile(project_output, audio = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
